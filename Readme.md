# Optimizer Playground
This is a set of manually-implemented optimization/gradient descent algorithms, typically used for machine learning:
- Vanilla/traditional gradient descent
- Momentum
- Nestorov momentum
- AdaGrad
- AdaDelta
- RMSProp
- Adam
- AdamW

They are built using only numpy as an exercise. The gradients are estimated by sampling the value of the target function at nearby points.
